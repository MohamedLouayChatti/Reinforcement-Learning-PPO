{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0d54c99",
   "metadata": {},
   "source": [
    "**Adding libraries and dependencies**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f11e35f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from mlagents_envs.environment import UnityEnvironment\n",
    "from mlagents_envs.side_channel.engine_configuration_channel import EngineConfigurationChannel\n",
    "from mlagents_envs.base_env import ActionTuple\n",
    "from collections import deque\n",
    "\n",
    "import tf_agents\n",
    "from tf_agents.environments import py_environment\n",
    "from tf_agents.environments import tf_py_environment\n",
    "from tf_agents.networks import q_network\n",
    "from tf_agents.networks import sequential\n",
    "from tf_agents.agents.dqn import dqn_agent\n",
    "from tf_agents.utils import common\n",
    "from tf_agents.policies import random_tf_policy\n",
    "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
    "from tf_agents.trajectories import trajectory\n",
    "from tf_agents.specs import array_spec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ff98d1",
   "metadata": {},
   "source": [
    "Checking GPU detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d7d874e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "print(tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e79e48b5",
   "metadata": {},
   "source": [
    "**Establishing connection with Unity environment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "88d95eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine_config_channel = EngineConfigurationChannel()\n",
    "\n",
    "env = UnityEnvironment(\n",
    "    file_name=None,\n",
    "    worker_id=0,\n",
    "    base_port=5004,\n",
    "    side_channels=[engine_config_channel]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7745b4e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Behavior: RobotBehaviour?team=0\n",
      "Action branches: (5,)\n",
      "Observation shapes: [(36,), (8,)]\n",
      "Active agents: 1\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "behavior_names = list(env.behavior_specs)\n",
    "behavior_name = behavior_names[0]\n",
    "behavior_spec = env.behavior_specs[behavior_name]\n",
    "\n",
    "print(f\"Behavior: {behavior_name}\")\n",
    "print(f\"Action branches: {behavior_spec.action_spec.discrete_branches}\")\n",
    "print(f\"Observation shapes: {[obs.shape for obs in behavior_spec.observation_specs]}\")\n",
    "\n",
    "decision_steps, terminal_steps = env.get_steps(behavior_name)\n",
    "print(f\"Active agents: {len(decision_steps)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "751d26ee",
   "metadata": {},
   "source": [
    "### Action Space\n",
    "- **5 discrete actions** corresponding to possible movements:\n",
    "  - `0`: Do nothing\n",
    "  - `1`: Go right  \n",
    "  - `2`: Go left\n",
    "  - `3`: Go forward\n",
    "  - `4`: Go backwards\n",
    "\n",
    "### Observation Space\n",
    "\n",
    "#### Ray Perception (36 dimensions)\n",
    "- **36 values** from ML-Agents Ray Perception Sensor 3D component\n",
    "- Detects sphere tags and distances in the environment\n",
    "\n",
    "#### Agent State (8 dimensions)\n",
    "- **8 values** divided as follows:\n",
    "  - Position: `x`, `y`, `z` coordinates\n",
    "  - Velocity: `x`, `y`, `z` velocity components  \n",
    "  - Rotation: `y`-axis rotation (facing direction)\n",
    "  - Time: Elapsed time since episode start\n",
    "\n",
    "**Total observation size:** 44\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "17cd20f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "action_size = 5\n",
    "state_size = 44"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7955b847",
   "metadata": {},
   "source": [
    "Preparing a get_state function for model training and inference, that is responsible for getting the agent's state at the current step and returning information which will later be used by the model :\n",
    "- state (44,) : agent's state at the current step\n",
    "- reward (float) : reward of the current step\n",
    "- done (boolean) : whether the episode is terminated at the current step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b06a9cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_state():\n",
    "    decision_steps, terminal_steps = env.get_steps(behavior_name)\n",
    "    \n",
    "    if len(decision_steps) > 0:\n",
    "        obs1 = decision_steps.obs[0][0] #Ray perception sensor observations\n",
    "        obs2 = decision_steps.obs[1][0] #Agent state observations\n",
    "        state = np.concatenate([obs1, obs2]) #Combine observations for correct DQN input format\n",
    "        return state, decision_steps.reward[0], False\n",
    "    \n",
    "    elif len(terminal_steps) > 0:\n",
    "        # Episode ended\n",
    "        obs1 = terminal_steps.obs[0][0]\n",
    "        obs2 = terminal_steps.obs[1][0] \n",
    "        state = np.concatenate([obs1, obs2])\n",
    "        return state, terminal_steps.reward[0], True\n",
    "    \n",
    "    return None, 0, False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc13a5b4",
   "metadata": {},
   "source": [
    "**Making a random Neural Network**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3768e999",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(64, activation='relu', input_shape=(state_size,)),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dense(32, activation='relu'),\n",
    "    tf.keras.layers.Dense(action_size, activation='linear')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9992709f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_12 (Dense)            (None, 64)                2880      \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 128)               8320      \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, 32)                4128      \n",
      "                                                                 \n",
      " dense_15 (Dense)            (None, 5)                 165       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 15493 (60.52 KB)\n",
      "Trainable params: 15493 (60.52 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d540d39d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.12881191, -0.05349946,  0.0935366 ,  0.09197785,  0.20343368,\n",
       "        0.10075463, -0.1383694 ,  0.1697702 , -0.03485735,  0.06967925,\n",
       "       -0.20340176, -0.03939438, -0.03095512,  0.15369894, -0.22063127,\n",
       "        0.10697804, -0.19072337,  0.05371515, -0.22273828, -0.06253025,\n",
       "       -0.20050858,  0.09067322,  0.10090114,  0.14284231, -0.19373435,\n",
       "        0.22996522,  0.15430067, -0.02219802, -0.11690249,  0.19520639,\n",
       "       -0.21624936, -0.14423747, -0.18991616, -0.19250663, -0.08125176,\n",
       "        0.18753819,  0.1471392 , -0.09828235, -0.15206493,  0.08851804,\n",
       "       -0.1677104 ,  0.01645143, -0.17666745, -0.15252185], dtype=float32)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l1_weights = model.get_weights()[0]\n",
    "l1_weights[:,0] #weights of the first neuron of the first layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30678d9b",
   "metadata": {},
   "source": [
    "Running the model with the initial random weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "33ad2d7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0: Action 3, Reward 0.00\n",
      "Step 10: Action 3, Reward 0.00\n",
      "Step 20: Action 3, Reward 0.00\n",
      "Step 30: Action 3, Reward 0.00\n",
      "Step 40: Action 3, Reward 0.00\n",
      "Step 50: Action 2, Reward 0.00\n",
      "Step 60: Action 4, Reward 0.00\n",
      "Step 70: Action 4, Reward 0.00\n",
      "Step 80: Action 4, Reward 0.00\n",
      "Step 90: Action 4, Reward 0.00\n"
     ]
    }
   ],
   "source": [
    "for step in range(100): #Run for 100 steps\n",
    "    state, reward, done = get_state()\n",
    "    \n",
    "    if state is not None:\n",
    "        if done:\n",
    "            env.reset()\n",
    "        q_values = model.predict(np.expand_dims(state, axis=0), verbose=0)\n",
    "        action = np.argmax(q_values[0])  # Choose best action\n",
    "        \n",
    "        if step % 10 == 0: \n",
    "            print(f\"Step {step}: Action {action}, Reward {reward:.2f}\")\n",
    "        \n",
    "        # Send action to Unity\n",
    "        action_tuple = ActionTuple(discrete=np.array([[action]]))\n",
    "        env.set_actions(behavior_name, action_tuple)\n",
    "        env.step()\n",
    "    else:\n",
    "        env.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0286db8",
   "metadata": {},
   "source": [
    "**Creating environment wrapper for TF agents**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9117ce3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnvironmentWrapper(py_environment.PyEnvironment):\n",
    "    def __init__(self, unity_env, behavior_name):\n",
    "        super().__init__()\n",
    "        self.unity_env = unity_env\n",
    "        self.behavior_name = behavior_name\n",
    "        \n",
    "        self._action_spec = array_spec.BoundedArraySpec(\n",
    "            shape=(),\n",
    "            dtype=np.int32,\n",
    "            minimum=0,\n",
    "            maximum=4,\n",
    "            name='action'\n",
    "        )\n",
    "        self._observation_spec = array_spec.ArraySpec(\n",
    "            shape=(44,),\n",
    "            dtype=np.float32,\n",
    "            name='observation'\n",
    "        )\n",
    "        \n",
    "        self._state = np.zeros(44, dtype=np.float32)\n",
    "        self._episode_ended = False\n",
    "\n",
    "    def action_spec(self):\n",
    "        return self._action_spec\n",
    "\n",
    "    def observation_spec(self):\n",
    "        return self._observation_spec\n",
    "\n",
    "    def _reset(self):\n",
    "        self.unity_env.reset()\n",
    "        self._episode_ended = False\n",
    "        state, _, _ = get_state()\n",
    "        if state is not None:\n",
    "            self._state = state.astype(np.float32)\n",
    "        return tf_agents.trajectories.time_step.restart(self._state)\n",
    "\n",
    "    def _step(self, action):\n",
    "        if self._episode_ended:\n",
    "            return self.reset()\n",
    "        \n",
    "        # Send action to Unity\n",
    "        action_tuple = ActionTuple(discrete=np.array([[action]]))\n",
    "        self.unity_env.set_actions(self.behavior_name, action_tuple)\n",
    "        self.unity_env.step()\n",
    "        \n",
    "        # Get new state\n",
    "        state, reward, done = get_state()\n",
    "        \n",
    "        if state is not None:\n",
    "            self._state = state.astype(np.float32)\n",
    "        \n",
    "        if done:\n",
    "            self._episode_ended = True\n",
    "            return tf_agents.trajectories.time_step.termination(self._state, reward)\n",
    "        else:\n",
    "            return tf_agents.trajectories.time_step.transition(self._state, reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e94b1465",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_wrapper = EnvironmentWrapper(env, behavior_name)\n",
    "tf_env = tf_py_environment.TFPyEnvironment(env_wrapper)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2157f31",
   "metadata": {},
   "source": [
    "**Setting the architecture of the Q-neural-networks**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1d0873d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "q_net = q_network.QNetwork(\n",
    "    tf_env.observation_spec(),\n",
    "    tf_env.action_spec(),\n",
    "    fc_layer_params=(64, 128, 32)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "352f9818",
   "metadata": {},
   "source": [
    "**Creating the agent (Deep-Q-Network)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "68a27375",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_step_counter = tf.Variable(0)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "agent = dqn_agent.DqnAgent(\n",
    "    tf_env.time_step_spec(),\n",
    "    tf_env.action_spec(),\n",
    "    q_network=q_net,\n",
    "    optimizer=optimizer,\n",
    "    td_errors_loss_fn=common.element_wise_squared_loss,\n",
    "    train_step_counter=train_step_counter,\n",
    "    epsilon_greedy=0.1,\n",
    "    target_update_period=100,\n",
    "    gamma=0.99 \n",
    ")\n",
    "\n",
    "agent.initialize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8339232",
   "metadata": {},
   "source": [
    "**Creating the replay buffer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "446ee226",
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
    "    data_spec=agent.collect_data_spec,\n",
    "    batch_size=tf_env.batch_size,\n",
    "    max_length=100000\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b87fd74",
   "metadata": {},
   "source": [
    "**Creating policies**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "45e6effa",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_policy = agent.policy # greedy: for testing and inference\n",
    "collect_policy = agent.collect_policy # Epsilon-greedy: for training and exploration\n",
    "random_policy = random_tf_policy.RandomTFPolicy(  # Completely random: for initial data collection\n",
    "    tf_env.time_step_spec(), \n",
    "    tf_env.action_spec()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "559531be",
   "metadata": {},
   "source": [
    "**Data collection**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "442cdd92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_data(policy, steps, speed):\n",
    "    engine_config_channel.set_configuration_parameters(time_scale=speed)\n",
    "    tf_env.reset()\n",
    "    for i in range(steps):\n",
    "        time_step = tf_env.current_time_step()\n",
    "        action_step = policy.action(time_step)\n",
    "        next_time_step = tf_env.step(action_step.action)\n",
    "        traj = trajectory.from_transition(time_step, action_step, next_time_step)\n",
    "        replay_buffer.add_batch(traj)\n",
    "        if i % 100 == 0:\n",
    "            print(f\"Collected {i} steps\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4352f2e4",
   "metadata": {},
   "source": [
    "Collecting initial data randomly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0e3a821b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collected 0 steps\n",
      "Collected 100 steps\n",
      "Collected 200 steps\n",
      "Collected 300 steps\n",
      "Collected 400 steps\n",
      "Collected 500 steps\n",
      "Collected 600 steps\n",
      "Collected 700 steps\n",
      "Collected 800 steps\n",
      "Collected 900 steps\n"
     ]
    }
   ],
   "source": [
    "collect_data(random_policy, 1000, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1185483",
   "metadata": {},
   "source": [
    "**Actual training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "092f3f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_iterations = 10000\n",
    "collect_steps_per_iteration = 4\n",
    "batch_size = 64\n",
    "log_interval = 200\n",
    "\n",
    "dataset = replay_buffer.as_dataset(\n",
    "    num_parallel_calls=3,\n",
    "    sample_batch_size=batch_size,\n",
    "    num_steps=2\n",
    ").prefetch(3)\n",
    "\n",
    "iterator = iter(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ad6a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "episode_returns = []\n",
    "\n",
    "for iteration in range(num_iterations):\n",
    "    #collect data for 4 steps\n",
    "    collect_data(collect_policy, collect_steps_per_iteration, 10)\n",
    "\n",
    "    # Sample a random batch from the replay buffer and train the agent\n",
    "    experience, _ = next(iterator)\n",
    "    train_loss = agent.train(experience).loss\n",
    "    \n",
    "    training_step = agent.train_step_counter.numpy()\n",
    "\n",
    "    # Log and evaluate each 200 training step\n",
    "    if training_step % log_interval == 0:\n",
    "        print(f'Training step {training_step}: Loss = {train_loss:.4f} | Env step = {iteration * collect_steps_per_iteration}')\n",
    "        train_losses.append(train_loss.numpy())\n",
    "        \n",
    "        time_step = tf_env.reset()\n",
    "        episode_return = 0.0\n",
    "        steps_in_episode = 0\n",
    "\n",
    "        while not time_step.is_last() and steps_in_episode < 500:\n",
    "            action_step = eval_policy.action(time_step)\n",
    "            time_step = tf_env.step(action_step.action)\n",
    "            episode_return += time_step.reward\n",
    "            steps_in_episode += 1\n",
    "            \n",
    "        episode_returns.append(episode_return.numpy()[0])\n",
    "        print(f'Episode Return: {episode_return.numpy()[0]:.2f} (Steps: {steps_in_episode})')\n",
    "        print(\"-\" * 30)\n",
    "\n",
    "print(\"Training complete\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DQL_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
