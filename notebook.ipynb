{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0d54c99",
   "metadata": {},
   "source": [
    "**Adding libraries and dependencies**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f11e35f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from mlagents_envs.environment import UnityEnvironment\n",
    "from mlagents_envs.side_channel.engine_configuration_channel import EngineConfigurationChannel\n",
    "from mlagents_envs.base_env import ActionTuple\n",
    "from collections import deque\n",
    "import random\n",
    "from tensorflow.keras import layers, Model\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e79e48b5",
   "metadata": {},
   "source": [
    "**Establishing connection with Unity environment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "88d95eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine_config_channel = EngineConfigurationChannel()\n",
    "\n",
    "env = UnityEnvironment(\n",
    "    file_name=None,\n",
    "    worker_id=0,\n",
    "    base_port=5004,\n",
    "    side_channels=[engine_config_channel]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7745b4e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Behavior: RobotBehaviour?team=0\n",
      "Action branches: (5,)\n",
      "Observation shapes: [(36,), (8,)]\n",
      "Active agents: 1\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "behavior_names = list(env.behavior_specs)\n",
    "behavior_name = behavior_names[0]\n",
    "behavior_spec = env.behavior_specs[behavior_name]\n",
    "\n",
    "print(f\"Behavior: {behavior_name}\")\n",
    "print(f\"Action branches: {behavior_spec.action_spec.discrete_branches}\")\n",
    "print(f\"Observation shapes: {[obs.shape for obs in behavior_spec.observation_specs]}\")\n",
    "\n",
    "decision_steps, terminal_steps = env.get_steps(behavior_name)\n",
    "print(f\"Active agents: {len(decision_steps)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "751d26ee",
   "metadata": {},
   "source": [
    "### Action Space\n",
    "- **5 discrete actions** corresponding to possible movements:\n",
    "  - `0`: Do nothing\n",
    "  - `1`: Go right  \n",
    "  - `2`: Go left\n",
    "  - `3`: Go forward\n",
    "  - `4`: Go backwards\n",
    "\n",
    "### Observation Space\n",
    "\n",
    "#### Ray Perception (36 dimensions)\n",
    "- **36 values** from ML-Agents Ray Perception Sensor 3D component\n",
    "- Detects sphere tags and distances in the environment\n",
    "\n",
    "#### Agent State (8 dimensions)\n",
    "- **8 values** divided as follows:\n",
    "  - Position: `x`, `y`, `z` coordinates\n",
    "  - Velocity: `x`, `y`, `z` velocity components  \n",
    "  - Rotation: `y`-axis rotation (facing direction)\n",
    "  - Time: Elapsed time since episode start\n",
    "\n",
    "**Total observation size:** 44\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "17cd20f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "action_size = 5\n",
    "state_size = 44"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7955b847",
   "metadata": {},
   "source": [
    "Preparing a get_state function for model training and inference, that is responsible for getting the agent's state at the current step and returning information which will later be used by the model :\n",
    "- state (44,) : agent's state at the current step\n",
    "- reward (float) : reward of the current step\n",
    "- done (boolean) : whether the episode is terminated at the current step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b06a9cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_state():\n",
    "    decision_steps, terminal_steps = env.get_steps(behavior_name)\n",
    "    \n",
    "    if len(decision_steps) > 0:\n",
    "        obs1 = decision_steps.obs[0][0] #Ray perception sensor observations\n",
    "        obs2 = decision_steps.obs[1][0] #Agent state observations\n",
    "        state = np.concatenate([obs1, obs2]) #Combine observations for correct DQN input format\n",
    "        return state, decision_steps.reward[0], False\n",
    "    \n",
    "    elif len(terminal_steps) > 0:\n",
    "        # Episode ended\n",
    "        obs1 = terminal_steps.obs[0][0]\n",
    "        obs2 = terminal_steps.obs[1][0] \n",
    "        state = np.concatenate([obs1, obs2])\n",
    "        return state, terminal_steps.reward[0], True\n",
    "    \n",
    "    return None, 0, False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc13a5b4",
   "metadata": {},
   "source": [
    "**Making the Deep-Q-Network**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3768e999",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/louay/miniforge3/envs/DQL_env/lib/python3.10/site-packages/keras/src/layers/core/dense.py:92: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(64, activation='relu', input_shape=(state_size,)),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dense(32, activation='relu'),\n",
    "    tf.keras.layers.Dense(action_size, activation='linear')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9992709f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_2\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_2\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,880</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,320</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">4,128</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">165</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_8 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m2,880\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_9 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │         \u001b[38;5;34m8,320\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_10 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │         \u001b[38;5;34m4,128\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_11 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m)              │           \u001b[38;5;34m165\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">15,493</span> (60.52 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m15,493\u001b[0m (60.52 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">15,493</span> (60.52 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m15,493\u001b[0m (60.52 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d540d39d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.18638815,  0.22980653, -0.20528173,  0.16089182,  0.15823342,\n",
       "        0.16693343, -0.02030241,  0.01310329,  0.0308039 , -0.11465297,\n",
       "       -0.05481113,  0.0300891 ,  0.06479736,  0.13707413,  0.01976277,\n",
       "        0.15838932,  0.10911919, -0.11471327, -0.09875946,  0.11628775,\n",
       "        0.01441787,  0.17267524,  0.15704541,  0.03268729,  0.09555544,\n",
       "       -0.21876311, -0.10274138,  0.09107761,  0.04657273,  0.03278114,\n",
       "        0.20651777, -0.06495936,  0.02366601,  0.20500664,  0.02424057,\n",
       "        0.15620904, -0.15711457, -0.18866795, -0.00094914,  0.22571234,\n",
       "        0.15795566, -0.00803611,  0.2254603 , -0.18097895], dtype=float32)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l1_weights = model.get_weights()[0]\n",
    "l1_weights[:,0] #weights of the first neuron of the first layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30678d9b",
   "metadata": {},
   "source": [
    "Running the model with the initial random weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "33ad2d7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0: Action 0, Reward 0.00\n",
      "Step 1: Action 0, Reward 0.00\n",
      "Step 2: Action 0, Reward 0.00\n",
      "Step 3: Action 0, Reward 0.00\n",
      "Step 4: Action 0, Reward 0.00\n",
      "Step 5: Action 0, Reward 0.00\n",
      "Step 6: Action 0, Reward 0.00\n",
      "Step 7: Action 0, Reward 0.00\n",
      "Step 8: Action 0, Reward 0.00\n",
      "Step 9: Action 0, Reward 0.00\n",
      "Step 10: Action 0, Reward 0.00\n",
      "Step 11: Action 0, Reward 0.00\n",
      "Step 12: Action 0, Reward 0.00\n",
      "Step 13: Action 0, Reward 0.00\n",
      "Step 14: Action 0, Reward 0.00\n",
      "Step 15: Action 0, Reward 0.00\n",
      "Step 16: Action 0, Reward 0.00\n",
      "Step 17: Action 0, Reward 0.00\n",
      "Step 18: Action 0, Reward 0.00\n",
      "Step 19: Action 0, Reward 0.00\n",
      "Step 20: Action 0, Reward 0.00\n",
      "Step 21: Action 0, Reward 0.00\n",
      "Step 22: Action 0, Reward 0.00\n",
      "Step 23: Action 0, Reward 0.00\n",
      "Step 24: Action 0, Reward 0.00\n",
      "Step 25: Action 0, Reward 0.00\n",
      "Step 26: Action 0, Reward 0.00\n",
      "Step 27: Action 0, Reward 0.00\n",
      "Step 28: Action 0, Reward 0.00\n",
      "Step 29: Action 0, Reward 0.00\n",
      "Step 30: Action 0, Reward 0.00\n",
      "Step 31: Action 0, Reward 0.00\n",
      "Step 32: Action 0, Reward 0.00\n",
      "Step 33: Action 0, Reward 0.00\n",
      "Step 34: Action 0, Reward 0.00\n",
      "Step 35: Action 0, Reward 0.00\n",
      "Step 36: Action 0, Reward 0.00\n",
      "Step 37: Action 0, Reward 0.00\n",
      "Step 38: Action 0, Reward 0.00\n",
      "Step 39: Action 0, Reward 0.00\n",
      "Step 40: Action 0, Reward 0.00\n",
      "Step 41: Action 0, Reward 0.00\n",
      "Step 42: Action 0, Reward 0.00\n",
      "Step 43: Action 0, Reward 0.00\n",
      "Step 44: Action 0, Reward 0.00\n",
      "Step 45: Action 0, Reward 0.00\n",
      "Step 46: Action 0, Reward 0.00\n",
      "Step 47: Action 0, Reward 0.00\n",
      "Step 48: Action 0, Reward 0.00\n",
      "Step 49: Action 0, Reward 0.00\n",
      "Step 50: Action 0, Reward 0.00\n",
      "Step 51: Action 0, Reward 0.00\n",
      "Step 52: Action 0, Reward 0.00\n",
      "Step 53: Action 0, Reward 0.00\n",
      "Step 54: Action 0, Reward 0.00\n",
      "Step 55: Action 0, Reward 0.00\n",
      "Step 56: Action 0, Reward 0.00\n",
      "Step 57: Action 0, Reward 0.00\n",
      "Step 58: Action 0, Reward 0.00\n",
      "Step 59: Action 0, Reward 0.00\n",
      "Step 60: Action 0, Reward 0.00\n",
      "Step 61: Action 0, Reward 0.00\n",
      "Step 62: Action 0, Reward 0.00\n",
      "Step 63: Action 0, Reward 0.00\n",
      "Step 64: Action 0, Reward 0.00\n",
      "Step 65: Action 0, Reward 0.00\n",
      "Step 66: Action 0, Reward 0.00\n",
      "Step 67: Action 0, Reward 0.00\n",
      "Step 68: Action 0, Reward 0.00\n",
      "Step 69: Action 0, Reward 0.00\n",
      "Step 70: Action 0, Reward 0.00\n",
      "Step 71: Action 0, Reward 0.00\n",
      "Step 72: Action 0, Reward 0.00\n",
      "Step 73: Action 0, Reward 0.00\n",
      "Step 74: Action 0, Reward 0.00\n",
      "Step 75: Action 0, Reward 0.00\n",
      "Step 76: Action 0, Reward 0.00\n",
      "Step 77: Action 0, Reward 0.00\n",
      "Step 78: Action 0, Reward 0.00\n",
      "Step 79: Action 0, Reward 0.00\n",
      "Step 80: Action 0, Reward 0.00\n",
      "Step 81: Action 0, Reward 0.00\n",
      "Step 82: Action 0, Reward 0.00\n",
      "Step 83: Action 0, Reward 0.00\n",
      "Step 84: Action 0, Reward 0.00\n",
      "Step 85: Action 0, Reward 0.00\n",
      "Step 86: Action 0, Reward 0.00\n",
      "Step 87: Action 0, Reward 0.00\n",
      "Step 88: Action 0, Reward 0.00\n",
      "Step 89: Action 0, Reward 0.00\n",
      "Step 90: Action 0, Reward 0.00\n",
      "Step 91: Action 0, Reward 0.00\n",
      "Step 92: Action 0, Reward 0.00\n",
      "Step 93: Action 0, Reward 0.00\n",
      "Step 94: Action 0, Reward 0.00\n",
      "Step 95: Action 0, Reward 0.00\n",
      "Step 96: Action 0, Reward 0.00\n",
      "Step 97: Action 0, Reward 0.00\n",
      "Step 98: Action 0, Reward 0.00\n",
      "Step 99: Action 0, Reward 0.00\n",
      "Step 100: Action 0, Reward 0.00\n",
      "Step 101: Action 0, Reward 0.00\n",
      "Step 102: Action 0, Reward 0.00\n",
      "Step 103: Action 0, Reward 0.00\n",
      "Step 104: Action 0, Reward 0.00\n",
      "Step 105: Action 0, Reward 0.00\n",
      "Step 106: Action 0, Reward 0.00\n",
      "Step 107: Action 0, Reward 0.00\n",
      "Step 108: Action 0, Reward 0.00\n",
      "Step 109: Action 0, Reward 0.00\n",
      "Step 110: Action 0, Reward 0.00\n",
      "Step 111: Action 0, Reward 0.00\n",
      "Step 112: Action 0, Reward 0.00\n",
      "Step 113: Action 0, Reward 0.00\n",
      "Step 114: Action 0, Reward 0.00\n",
      "Step 115: Action 0, Reward 0.00\n",
      "Step 116: Action 0, Reward 0.00\n",
      "Step 117: Action 0, Reward 0.00\n",
      "Step 118: Action 0, Reward 0.00\n",
      "Step 119: Action 0, Reward 0.00\n",
      "Step 120: Action 0, Reward 0.00\n",
      "Step 121: Action 0, Reward 0.00\n",
      "Step 122: Action 0, Reward 0.00\n",
      "Step 123: Action 0, Reward 0.00\n",
      "Step 124: Action 0, Reward 0.00\n",
      "Step 125: Action 0, Reward 0.00\n",
      "Step 126: Action 0, Reward 0.00\n",
      "Step 127: Action 0, Reward 0.00\n",
      "Step 128: Action 0, Reward 0.00\n",
      "Step 129: Action 0, Reward 0.00\n",
      "Step 130: Action 0, Reward 0.00\n",
      "Step 131: Action 0, Reward 0.00\n",
      "Step 132: Action 0, Reward 0.00\n",
      "Step 133: Action 0, Reward 0.00\n",
      "Step 134: Action 0, Reward 0.00\n",
      "Step 135: Action 0, Reward 0.00\n",
      "Step 136: Action 0, Reward 0.00\n",
      "Step 137: Action 0, Reward 0.00\n",
      "Step 138: Action 0, Reward 0.00\n",
      "Step 139: Action 0, Reward 0.00\n",
      "Step 140: Action 0, Reward 0.00\n",
      "Step 141: Action 0, Reward 0.00\n",
      "Step 142: Action 0, Reward 0.00\n",
      "Step 143: Action 0, Reward 0.00\n",
      "Step 144: Action 0, Reward 0.00\n",
      "Step 145: Action 0, Reward 0.00\n",
      "Step 146: Action 0, Reward 0.00\n",
      "Step 147: Action 0, Reward 0.00\n",
      "Step 148: Action 0, Reward 0.00\n",
      "Step 149: Action 0, Reward 0.00\n",
      "Step 150: Action 0, Reward 0.00\n",
      "Step 151: Action 0, Reward 0.00\n",
      "Step 152: Action 0, Reward 0.00\n",
      "Step 153: Action 0, Reward 0.00\n",
      "Step 154: Action 0, Reward 0.00\n",
      "Step 155: Action 0, Reward 0.00\n",
      "Step 156: Action 0, Reward 0.00\n",
      "Step 157: Action 0, Reward 0.00\n",
      "Step 158: Action 0, Reward 0.00\n",
      "Step 159: Action 0, Reward 0.00\n",
      "Step 160: Action 0, Reward 0.00\n",
      "Step 161: Action 0, Reward 0.00\n",
      "Step 162: Action 0, Reward 0.00\n",
      "Step 163: Action 0, Reward 0.00\n",
      "Step 164: Action 0, Reward 0.00\n",
      "Step 165: Action 0, Reward 0.00\n",
      "Step 166: Action 0, Reward 0.00\n",
      "Step 167: Action 0, Reward 0.00\n",
      "Step 168: Action 0, Reward 0.00\n",
      "Step 169: Action 0, Reward 0.00\n",
      "Step 170: Action 0, Reward 0.00\n",
      "Step 171: Action 0, Reward 0.00\n",
      "Step 172: Action 0, Reward 0.00\n",
      "Step 173: Action 0, Reward 0.00\n",
      "Step 174: Action 0, Reward 0.00\n",
      "Step 175: Action 0, Reward 0.00\n",
      "Step 176: Action 0, Reward 0.00\n",
      "Step 177: Action 0, Reward 0.00\n",
      "Step 178: Action 0, Reward 0.00\n",
      "Step 179: Action 0, Reward 0.00\n",
      "Step 180: Action 0, Reward 0.00\n",
      "Step 181: Action 0, Reward 0.00\n",
      "Step 182: Action 0, Reward 0.00\n",
      "Step 183: Action 0, Reward 0.00\n",
      "Step 184: Action 0, Reward 0.00\n",
      "Step 185: Action 0, Reward 0.00\n",
      "Step 186: Action 0, Reward 0.00\n",
      "Step 187: Action 0, Reward 0.00\n",
      "Step 188: Action 0, Reward 0.00\n",
      "Step 189: Action 0, Reward 0.00\n",
      "Step 190: Action 0, Reward 0.00\n",
      "Step 191: Action 0, Reward 0.00\n",
      "Step 192: Action 0, Reward 0.00\n",
      "Step 193: Action 0, Reward 0.00\n",
      "Step 194: Action 0, Reward 0.00\n",
      "Step 195: Action 0, Reward 0.00\n",
      "Step 196: Action 0, Reward 0.00\n",
      "Step 197: Action 0, Reward 0.00\n",
      "Step 198: Action 0, Reward 0.00\n",
      "Step 199: Action 0, Reward 0.00\n",
      "Step 200: Action 0, Reward 0.00\n",
      "Step 201: Action 0, Reward 0.00\n",
      "Step 202: Action 0, Reward 0.00\n",
      "Step 203: Action 0, Reward 0.00\n",
      "Step 204: Action 0, Reward 0.00\n",
      "Step 205: Action 0, Reward 0.00\n",
      "Step 206: Action 0, Reward 0.00\n",
      "Step 207: Action 0, Reward 0.00\n",
      "Step 208: Action 0, Reward 0.00\n",
      "Step 209: Action 0, Reward 0.00\n",
      "Step 210: Action 0, Reward 0.00\n",
      "Step 211: Action 0, Reward 0.00\n",
      "Step 212: Action 0, Reward 0.00\n",
      "Step 213: Action 0, Reward 0.00\n",
      "Step 214: Action 0, Reward 0.00\n",
      "Step 215: Action 0, Reward 0.00\n",
      "Step 216: Action 0, Reward 0.00\n",
      "Step 217: Action 0, Reward 0.00\n",
      "Step 218: Action 0, Reward 0.00\n",
      "Step 219: Action 0, Reward 0.00\n",
      "Step 220: Action 0, Reward 0.00\n",
      "Step 221: Action 0, Reward 0.00\n",
      "Step 222: Action 0, Reward 0.00\n",
      "Step 223: Action 0, Reward 0.00\n",
      "Step 224: Action 0, Reward 0.00\n",
      "Step 225: Action 0, Reward 0.00\n",
      "Step 226: Action 0, Reward 0.00\n",
      "Step 227: Action 0, Reward 0.00\n",
      "Step 228: Action 0, Reward 0.00\n",
      "Step 229: Action 0, Reward 0.00\n",
      "Step 230: Action 0, Reward 0.00\n",
      "Step 231: Action 0, Reward 0.00\n",
      "Step 232: Action 0, Reward 0.00\n",
      "Step 233: Action 0, Reward 0.00\n",
      "Step 234: Action 0, Reward 0.00\n",
      "Step 235: Action 0, Reward 0.00\n",
      "Step 236: Action 0, Reward 0.00\n",
      "Step 237: Action 0, Reward 0.00\n",
      "Step 238: Action 0, Reward 0.00\n",
      "Step 239: Action 0, Reward 0.00\n",
      "Step 240: Action 0, Reward 0.00\n",
      "Step 241: Action 0, Reward 0.00\n",
      "Step 242: Action 0, Reward 0.00\n",
      "Step 243: Action 0, Reward 0.00\n",
      "Step 244: Action 0, Reward 0.00\n",
      "Step 245: Action 0, Reward 0.00\n",
      "Step 246: Action 0, Reward 0.00\n",
      "Step 247: Action 0, Reward 0.00\n",
      "Step 248: Action 0, Reward 0.00\n",
      "Step 249: Action 0, Reward 0.00\n",
      "Step 250: Action 0, Reward 0.00\n",
      "Step 251: Action 0, Reward 0.00\n",
      "Step 252: Action 0, Reward 0.00\n",
      "Step 253: Action 0, Reward 0.00\n",
      "Step 254: Action 0, Reward 0.00\n",
      "Step 255: Action 0, Reward 0.00\n",
      "Step 256: Action 0, Reward 0.00\n",
      "Step 257: Action 0, Reward 0.00\n",
      "Step 258: Action 0, Reward 0.00\n",
      "Step 259: Action 0, Reward 0.00\n",
      "Step 260: Action 0, Reward 0.00\n",
      "Step 261: Action 0, Reward 0.00\n",
      "Step 262: Action 0, Reward 0.00\n",
      "Step 263: Action 0, Reward 0.00\n",
      "Step 264: Action 0, Reward 0.00\n",
      "Step 265: Action 0, Reward 0.00\n",
      "Step 266: Action 0, Reward 0.00\n",
      "Step 267: Action 0, Reward 0.00\n",
      "Step 268: Action 0, Reward 0.00\n",
      "Step 269: Action 0, Reward 0.00\n",
      "Step 270: Action 0, Reward 0.00\n",
      "Step 271: Action 0, Reward 0.00\n",
      "Step 272: Action 0, Reward 0.00\n",
      "Step 273: Action 0, Reward 0.00\n",
      "Step 274: Action 0, Reward 0.00\n",
      "Step 275: Action 0, Reward 0.00\n",
      "Step 276: Action 0, Reward 0.00\n",
      "Step 277: Action 0, Reward 0.00\n",
      "Step 278: Action 0, Reward 0.00\n",
      "Step 279: Action 0, Reward 0.00\n",
      "Step 280: Action 0, Reward 0.00\n",
      "Step 281: Action 0, Reward 0.00\n",
      "Step 282: Action 0, Reward 0.00\n",
      "Step 283: Action 0, Reward 0.00\n",
      "Step 284: Action 0, Reward 0.00\n",
      "Step 285: Action 0, Reward 0.00\n",
      "Step 286: Action 0, Reward 0.00\n",
      "Step 287: Action 0, Reward 0.00\n",
      "Step 288: Action 0, Reward 0.00\n",
      "Step 289: Action 0, Reward 0.00\n",
      "Step 290: Action 0, Reward 0.00\n",
      "Step 291: Action 0, Reward 0.00\n",
      "Step 292: Action 0, Reward 0.00\n",
      "Step 293: Action 0, Reward 0.00\n",
      "Step 294: Action 0, Reward 0.00\n",
      "Step 295: Action 0, Reward 0.00\n",
      "Step 296: Action 0, Reward 0.00\n",
      "Step 297: Action 0, Reward 0.00\n",
      "Step 298: Action 0, Reward 0.00\n",
      "Step 299: Action 0, Reward 0.00\n",
      "Step 300: Action 0, Reward 0.00\n",
      "Step 301: Action 0, Reward 0.00\n",
      "Step 302: Action 0, Reward 0.00\n",
      "Step 303: Action 0, Reward 0.00\n",
      "Step 304: Action 0, Reward 0.00\n",
      "Step 305: Action 0, Reward 0.00\n",
      "Step 306: Action 0, Reward 0.00\n",
      "Step 307: Action 0, Reward 0.00\n",
      "Step 308: Action 0, Reward 0.00\n",
      "Step 309: Action 0, Reward 0.00\n",
      "Step 310: Action 0, Reward 0.00\n",
      "Step 311: Action 0, Reward 0.00\n",
      "Step 312: Action 0, Reward 0.00\n",
      "Step 313: Action 0, Reward 0.00\n",
      "Step 314: Action 0, Reward 0.00\n",
      "Step 315: Action 0, Reward 0.00\n",
      "Step 316: Action 0, Reward 0.00\n",
      "Step 317: Action 0, Reward 0.00\n",
      "Step 318: Action 0, Reward 0.00\n",
      "Step 319: Action 0, Reward 0.00\n",
      "Step 320: Action 0, Reward 0.00\n",
      "Step 321: Action 0, Reward 0.00\n",
      "Step 322: Action 0, Reward 0.00\n",
      "Step 323: Action 0, Reward 0.00\n",
      "Step 324: Action 0, Reward 0.00\n",
      "Step 325: Action 0, Reward 0.00\n",
      "Step 326: Action 0, Reward 0.00\n",
      "Step 327: Action 0, Reward 0.00\n",
      "Step 328: Action 0, Reward 0.00\n",
      "Step 329: Action 0, Reward 0.00\n",
      "Step 330: Action 0, Reward 0.00\n",
      "Step 331: Action 0, Reward 0.00\n",
      "Step 332: Action 0, Reward 0.00\n",
      "Step 333: Action 0, Reward 0.00\n",
      "Step 334: Action 0, Reward 0.00\n",
      "Step 335: Action 0, Reward 0.00\n",
      "Step 336: Action 0, Reward 0.00\n",
      "Step 337: Action 0, Reward 0.00\n",
      "Step 338: Action 0, Reward 0.00\n",
      "Step 339: Action 0, Reward 0.00\n",
      "Step 340: Action 0, Reward 0.00\n",
      "Step 341: Action 0, Reward 0.00\n",
      "Step 342: Action 0, Reward 0.00\n",
      "Step 343: Action 0, Reward 0.00\n",
      "Step 344: Action 0, Reward 0.00\n",
      "Step 345: Action 0, Reward 0.00\n",
      "Step 346: Action 0, Reward 0.00\n",
      "Step 347: Action 0, Reward 0.00\n",
      "Step 348: Action 0, Reward 0.00\n",
      "Step 349: Action 0, Reward 0.00\n",
      "Step 350: Action 0, Reward 0.00\n",
      "Step 351: Action 0, Reward 0.00\n",
      "Step 352: Action 0, Reward 0.00\n",
      "Step 353: Action 0, Reward 0.00\n",
      "Step 354: Action 0, Reward 0.00\n",
      "Step 355: Action 0, Reward 0.00\n",
      "Step 356: Action 0, Reward 0.00\n",
      "Step 357: Action 0, Reward 0.00\n",
      "Step 358: Action 0, Reward 0.00\n",
      "Step 359: Action 0, Reward 0.00\n",
      "Step 360: Action 0, Reward 0.00\n",
      "Step 361: Action 0, Reward 0.00\n",
      "Step 362: Action 0, Reward 0.00\n",
      "Step 363: Action 0, Reward 0.00\n",
      "Step 364: Action 0, Reward 0.00\n",
      "Step 365: Action 0, Reward 0.00\n",
      "Step 366: Action 0, Reward 0.00\n",
      "Step 367: Action 0, Reward 0.00\n",
      "Step 368: Action 0, Reward 0.00\n"
     ]
    },
    {
     "ename": "UnityCommunicatorStoppedException",
     "evalue": "Communicator has exited.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnityCommunicatorStoppedException\u001b[0m         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m action_tuple \u001b[38;5;241m=\u001b[39m ActionTuple(discrete\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39marray([[action]]))\n\u001b[1;32m     12\u001b[0m env\u001b[38;5;241m.\u001b[39mset_actions(behavior_name, action_tuple)\n\u001b[0;32m---> 13\u001b[0m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m done:\n\u001b[1;32m     16\u001b[0m     env\u001b[38;5;241m.\u001b[39mreset()\n",
      "File \u001b[0;32m~/miniforge3/envs/DQL_env/lib/python3.10/site-packages/mlagents_envs/timers.py:305\u001b[0m, in \u001b[0;36mtimed.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    304\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m hierarchical_timer(func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m):\n\u001b[0;32m--> 305\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/DQL_env/lib/python3.10/site-packages/mlagents_envs/environment.py:350\u001b[0m, in \u001b[0;36mUnityEnvironment.step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    348\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_communicator\u001b[38;5;241m.\u001b[39mexchange(step_input, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_poll_process)\n\u001b[1;32m    349\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m outputs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 350\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m UnityCommunicatorStoppedException(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCommunicator has exited.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    351\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_behavior_specs(outputs)\n\u001b[1;32m    352\u001b[0m rl_output \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mrl_output\n",
      "\u001b[0;31mUnityCommunicatorStoppedException\u001b[0m: Communicator has exited."
     ]
    }
   ],
   "source": [
    "for step in range(1000):  # Run for 100 steps\n",
    "    state, reward, done = get_state()\n",
    "    \n",
    "    if state is not None:\n",
    "        q_values = model.predict(np.expand_dims(state, axis=0), verbose=0)\n",
    "        action = np.argmax(q_values[0])  # Choose best action\n",
    "        \n",
    "        print(f\"Step {step}: Action {action}, Reward {reward:.2f}\")\n",
    "        \n",
    "        # Send action to Unity\n",
    "        action_tuple = ActionTuple(discrete=np.array([[action]]))\n",
    "        env.set_actions(behavior_name, action_tuple)\n",
    "        env.step()\n",
    "        \n",
    "        if done:\n",
    "            env.reset()\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DQL_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
