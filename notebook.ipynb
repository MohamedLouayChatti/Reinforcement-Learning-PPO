{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0d54c99",
   "metadata": {},
   "source": [
    "**Adding libraries and dependencies**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f11e35f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from mlagents_envs.environment import UnityEnvironment\n",
    "from mlagents_envs.side_channel.engine_configuration_channel import EngineConfigurationChannel\n",
    "from mlagents_envs.base_env import ActionTuple\n",
    "from collections import deque\n",
    "\n",
    "import tf_agents\n",
    "from tf_agents.environments import py_environment\n",
    "from tf_agents.environments import tf_py_environment\n",
    "from tf_agents.networks import q_network\n",
    "from tf_agents.networks import sequential\n",
    "from tf_agents.agents.dqn import dqn_agent\n",
    "from tf_agents.utils import common\n",
    "from tf_agents.policies import random_tf_policy\n",
    "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
    "from tf_agents.trajectories import trajectory\n",
    "from tf_agents.specs import array_spec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ff98d1",
   "metadata": {},
   "source": [
    "Checking GPU detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d7d874e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "print(tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e79e48b5",
   "metadata": {},
   "source": [
    "**Establishing connection with Unity environment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "88d95eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine_config_channel = EngineConfigurationChannel()\n",
    "\n",
    "env = UnityEnvironment(\n",
    "    file_name=None,\n",
    "    worker_id=0,\n",
    "    base_port=5004,\n",
    "    side_channels=[engine_config_channel]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7745b4e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Behavior: RobotBehaviour?team=0\n",
      "Action branches: (5,)\n",
      "Observation shapes: [(36,), (8,)]\n",
      "Active agents: 1\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "behavior_names = list(env.behavior_specs)\n",
    "behavior_name = behavior_names[0]\n",
    "behavior_spec = env.behavior_specs[behavior_name]\n",
    "\n",
    "print(f\"Behavior: {behavior_name}\")\n",
    "print(f\"Action branches: {behavior_spec.action_spec.discrete_branches}\")\n",
    "print(f\"Observation shapes: {[obs.shape for obs in behavior_spec.observation_specs]}\")\n",
    "\n",
    "decision_steps, terminal_steps = env.get_steps(behavior_name)\n",
    "print(f\"Active agents: {len(decision_steps)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "751d26ee",
   "metadata": {},
   "source": [
    "### Action Space\n",
    "- **5 discrete actions** corresponding to possible movements:\n",
    "  - `0`: Do nothing\n",
    "  - `1`: Go right  \n",
    "  - `2`: Go left\n",
    "  - `3`: Go forward\n",
    "  - `4`: Go backwards\n",
    "\n",
    "### Observation Space\n",
    "\n",
    "#### Ray Perception (36 dimensions)\n",
    "- **36 values** from ML-Agents Ray Perception Sensor 3D component\n",
    "- Detects sphere tags and distances in the environment\n",
    "\n",
    "#### Agent State (8 dimensions)\n",
    "- **8 values** divided as follows:\n",
    "  - Position: `x`, `y`, `z` coordinates\n",
    "  - Velocity: `x`, `y`, `z` velocity components  \n",
    "  - Rotation: `y`-axis rotation (facing direction)\n",
    "  - Time: Elapsed time since episode start\n",
    "\n",
    "**Total observation size:** 44\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "17cd20f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "action_size = 5\n",
    "state_size = 44"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7955b847",
   "metadata": {},
   "source": [
    "Preparing a get_state function for model training and inference, that is responsible for getting the agent's state at the current step and returning information which will later be used by the model :\n",
    "- state (44,) : agent's state at the current step\n",
    "- reward (float) : reward of the current step\n",
    "- done (boolean) : whether the episode is terminated at the current step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b06a9cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_state():\n",
    "    decision_steps, terminal_steps = env.get_steps(behavior_name)\n",
    "    \n",
    "    if len(decision_steps) > 0:\n",
    "        obs1 = decision_steps.obs[0][0] #Ray perception sensor observations\n",
    "        obs2 = decision_steps.obs[1][0] #Agent state observations\n",
    "        state = np.concatenate([obs1, obs2]) #Combine observations for correct DQN input format\n",
    "        return state, decision_steps.reward[0], False\n",
    "    \n",
    "    elif len(terminal_steps) > 0:\n",
    "        # Episode ended\n",
    "        obs1 = terminal_steps.obs[0][0]\n",
    "        obs2 = terminal_steps.obs[1][0] \n",
    "        state = np.concatenate([obs1, obs2])\n",
    "        return state, terminal_steps.reward[0], True\n",
    "    \n",
    "    return None, 0, False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc13a5b4",
   "metadata": {},
   "source": [
    "**Making a random Neural Network**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3768e999",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-13 17:43:51.700135: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-08-13 17:43:51.700276: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-08-13 17:43:51.700305: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-08-13 17:43:51.866858: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-08-13 17:43:51.866931: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-08-13 17:43:51.866939: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2022] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2025-08-13 17:43:51.866967: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-08-13 17:43:51.866988: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 1767 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3050 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(64, activation='relu', input_shape=(state_size,)),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dense(32, activation='relu'),\n",
    "    tf.keras.layers.Dense(action_size, activation='linear')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9992709f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 64)                2880      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 128)               8320      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 32)                4128      \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 5)                 165       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 15493 (60.52 KB)\n",
      "Trainable params: 15493 (60.52 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d540d39d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.04756963, -0.18447664,  0.21211015,  0.16453399, -0.06873269,\n",
       "       -0.22401607, -0.02547716,  0.07611237,  0.12797199,  0.11523403,\n",
       "        0.03259476,  0.1020243 ,  0.07130145, -0.05244517,  0.10843535,\n",
       "        0.13519596,  0.0549029 ,  0.20682941, -0.18723108, -0.21134442,\n",
       "        0.15389724, -0.13932393,  0.04030319, -0.02738242,  0.1155277 ,\n",
       "       -0.07928608,  0.06979112,  0.1446536 , -0.18376745,  0.04264431,\n",
       "       -0.04889524, -0.22469243, -0.0410417 ,  0.10263161, -0.19351456,\n",
       "        0.05394004,  0.06007572,  0.13820879,  0.10831194, -0.13774943,\n",
       "        0.03126998, -0.15036657, -0.04459676, -0.14785582], dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l1_weights = model.get_weights()[0]\n",
    "l1_weights[:,0] #weights of the first neuron of the first layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30678d9b",
   "metadata": {},
   "source": [
    "Running the model with the initial random weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "33ad2d7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0: Action 1, Reward 0.00\n",
      "Step 10: Action 1, Reward 0.00\n",
      "Step 20: Action 3, Reward 0.00\n",
      "Step 30: Action 3, Reward 0.00\n",
      "Step 40: Action 1, Reward 0.00\n",
      "Step 50: Action 3, Reward 0.00\n",
      "Step 60: Action 1, Reward 0.00\n",
      "Step 70: Action 1, Reward 0.00\n"
     ]
    },
    {
     "ename": "UnityCommunicatorStoppedException",
     "evalue": "Communicator has exited.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnityCommunicatorStoppedException\u001b[0m         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 16\u001b[0m\n\u001b[1;32m     14\u001b[0m     action_tuple \u001b[38;5;241m=\u001b[39m ActionTuple(discrete\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39marray([[action]]))\n\u001b[1;32m     15\u001b[0m     env\u001b[38;5;241m.\u001b[39mset_actions(behavior_name, action_tuple)\n\u001b[0;32m---> 16\u001b[0m     \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     18\u001b[0m     env\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/miniforge3/envs/DQL_env/lib/python3.10/site-packages/mlagents_envs/timers.py:305\u001b[0m, in \u001b[0;36mtimed.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    304\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m hierarchical_timer(func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m):\n\u001b[0;32m--> 305\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/DQL_env/lib/python3.10/site-packages/mlagents_envs/environment.py:350\u001b[0m, in \u001b[0;36mUnityEnvironment.step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    348\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_communicator\u001b[38;5;241m.\u001b[39mexchange(step_input, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_poll_process)\n\u001b[1;32m    349\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m outputs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 350\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m UnityCommunicatorStoppedException(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCommunicator has exited.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    351\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_behavior_specs(outputs)\n\u001b[1;32m    352\u001b[0m rl_output \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mrl_output\n",
      "\u001b[0;31mUnityCommunicatorStoppedException\u001b[0m: Communicator has exited."
     ]
    }
   ],
   "source": [
    "for step in range(500): #Run for 500 steps\n",
    "    state, reward, done = get_state()\n",
    "    \n",
    "    if state is not None:\n",
    "        if done:\n",
    "            env.reset()\n",
    "        q_values = model.predict(np.expand_dims(state, axis=0), verbose=0)\n",
    "        action = np.argmax(q_values[0])  # Choose best action\n",
    "        \n",
    "        if step % 10 == 0: \n",
    "            print(f\"Step {step}: Action {action}, Reward {reward:.2f}\")\n",
    "        \n",
    "        # Send action to Unity\n",
    "        action_tuple = ActionTuple(discrete=np.array([[action]]))\n",
    "        env.set_actions(behavior_name, action_tuple)\n",
    "        env.step()\n",
    "    else:\n",
    "        env.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0286db8",
   "metadata": {},
   "source": [
    "**Creating environment wrapper for TF agents**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9117ce3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnvironmentWrapper(py_environment.PyEnvironment):\n",
    "    def __init__(self, unity_env, behavior_name):\n",
    "        super().__init__()\n",
    "        self.unity_env = unity_env\n",
    "        self.behavior_name = behavior_name\n",
    "        \n",
    "        self._action_spec = array_spec.BoundedArraySpec(\n",
    "            shape=(),\n",
    "            dtype=np.int32,\n",
    "            minimum=0,\n",
    "            maximum=4,\n",
    "            name='action'\n",
    "        )\n",
    "        self._observation_spec = array_spec.ArraySpec(\n",
    "            shape=(44,),\n",
    "            dtype=np.float32,\n",
    "            name='observation'\n",
    "        )\n",
    "        \n",
    "        self._state = np.zeros(44, dtype=np.float32)\n",
    "        self._episode_ended = False\n",
    "\n",
    "    def action_spec(self):\n",
    "        return self._action_spec\n",
    "\n",
    "    def observation_spec(self):\n",
    "        return self._observation_spec\n",
    "\n",
    "    def _reset(self):\n",
    "        self.unity_env.reset()\n",
    "        self._episode_ended = False\n",
    "        state, _, _ = get_state()\n",
    "        if state is not None:\n",
    "            self._state = state.astype(np.float32)\n",
    "        return tf_agents.trajectories.time_step.restart(self._state)\n",
    "\n",
    "    def _step(self, action):\n",
    "        if self._episode_ended:\n",
    "            return self.reset()\n",
    "        \n",
    "        # Send action to Unity\n",
    "        action_tuple = ActionTuple(discrete=np.array([[action]]))\n",
    "        self.unity_env.set_actions(self.behavior_name, action_tuple)\n",
    "        self.unity_env.step()\n",
    "        \n",
    "        # Get new state\n",
    "        state, reward, done = get_state()\n",
    "        \n",
    "        if state is not None:\n",
    "            self._state = state.astype(np.float32)\n",
    "        \n",
    "        if done:\n",
    "            self._episode_ended = True\n",
    "            return tf_agents.trajectories.time_step.termination(self._state, reward)\n",
    "        else:\n",
    "            return tf_agents.trajectories.time_step.transition(self._state, reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e94b1465",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_wrapper = EnvironmentWrapper(env, behavior_name)\n",
    "tf_env = tf_py_environment.TFPyEnvironment(env_wrapper)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2157f31",
   "metadata": {},
   "source": [
    "**Making the DQN with TF agents**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1d0873d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn = q_network.QNetwork(\n",
    "    tf_env.observation_spec(),\n",
    "    tf_env.action_spec(),\n",
    "    fc_layer_params=(64, 128, 32)\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DQL_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
